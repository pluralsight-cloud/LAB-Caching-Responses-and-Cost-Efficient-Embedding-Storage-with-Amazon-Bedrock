{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb296e7-e73d-4f87-b8a0-d24ee2386143",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Caching Responses and Cost-efficient Embedding Storage with Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9b917-3d88-48ae-bdbf-15ba3eeed506",
   "metadata": {},
   "source": [
    "## Objective Two: Implement Response Caching to Reduce Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3aedfc-5307-475e-be03-8e252e644bcd",
   "metadata": {},
   "source": [
    "In this hands-on walkthrough, you’ll use the `boto3 Python SDK` to interact with `Amazon Bedrock`, installing the required dependencies and setting up the `Bedrock` client. Once that’s done, you’ll craft a prompt to ensure that cache checkpoints are created, and then you’ll perform a model invocation using the initialized `Bedrock` client to validate whether cache checkpoints are created and whether you get a cache hit, reducing inference costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71363126-714c-4b93-9e2e-ba579705e676",
   "metadata": {},
   "source": [
    "### 1. Prepare the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc4904-c309-4335-b978-fa2e107b59db",
   "metadata": {},
   "source": [
    "This step includes the code to install the required `Python` packages needed for the rest of the exercise and restart the kernel to ensure the packages are properly loaded. While running, you might see some `pip` dependency warnings. These can be safely ignored as they won’t impact the steps we’re performing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c64c27-ed63-4183-b482-13ed8153b302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"✅ Please wait while the installation completes. This may take a few \"\n",
    "      \"minutes. If you encounter any dependency errors, you can ignore them.\")\n",
    "%pip install --upgrade -q botocore\n",
    "%pip install --upgrade -q boto3\n",
    "print(\"✅ Installation completed!\")\n",
    "print(\"✅ Restarting kernel\")\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "try:\n",
    "    display(HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\"))\n",
    "    print(\"✅ Kernel restarted successfully\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to restart the kernel\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327abcaa",
   "metadata": {},
   "source": [
    "In this step, you import `boto3` to interact with AWS services programmatically and `json` to handle `JSON` data formatting for requests and responses. You also create a `Bedrock` client using `boto3`, which allows you to interact with the `Amazon Bedrock` service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd67a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import boto3\n",
    "    import json\n",
    "    import time\n",
    "    print(\"----------------------------\")\n",
    "    print(\"✅ Libraries loaded successfully.\")\n",
    "except ImportError as e:\n",
    "    print(\"----------------------------\")\n",
    "    print(\"❌ Failed to load libraries.\")\n",
    "    print(f\"Error: {e}\")\n",
    "try:\n",
    "    client = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=\"us-east-1\"\n",
    "    )\n",
    "    print(\"✅ bedrock-runtime client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to initialize bedrock-runtime client.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f32e0b-389e-4445-9dc0-70445944dae4",
   "metadata": {},
   "source": [
    "### 2. Load Intructions and Privacy Statement Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c6b13-c212-4875-9544-af45b7dc4e3c",
   "metadata": {},
   "source": [
    "In this step, the contents of two local text files — `instructions.txt` and `privacy_statement.txt` — are loaded, normalized and stored in variables. This content will be used throughout the exercise to simulate user queries and test how the model responds to questions about the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d6f421-72a6-468c-b465-525e1f043039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to read file content and normalize newlines\n",
    "def read_file_content(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().replace('\\r\\n', '\\n').replace('\\r', '\\n').strip()\n",
    "            print(\"✅ File loaded successfully.\")\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ File not found. Please check the file path.\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(\"❌ Failed to read file.\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# File paths\n",
    "instructions_file = \"instructions.txt\"\n",
    "privacy_statement_file = \"privacy_statement.txt\"\n",
    "\n",
    "# Load content\n",
    "instructions_content = read_file_content(instructions_file)\n",
    "privacy_statement_content = read_file_content(privacy_statement_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b336aa2a-6c8a-48fd-9b35-94a17ffcab5a",
   "metadata": {},
   "source": [
    "### 3. Construct Prompt for Model Invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7eb9b7-1de1-4bf9-8325-b84aaa690bb6",
   "metadata": {},
   "source": [
    "In this step, we define a user inquiry and build a combined promt. To ensure cache checkpoints are created, the combined token count from the prompt and the response must exceed the 1000-token minimum required by the `Amazon Nova Lite` model. To help with this, we have loaded clear and detailed instructions from an `instructions.txt` file, along with the `privacy_statement.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d143cda-5b0c-4116-bacd-06831694c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the static part of the prompt\n",
    "static_prompt_content = (\n",
    "    f\"--- BEGIN INSTRUCTIONS ---\\n{instructions_content}\\n--- END INSTRUCTIONS ---\\n\\n\"\n",
    "    f\"--- BEGIN PRIVACY STATEMENT ---\\n{privacy_statement_content}\\n--- END PRIVACY STATEMENT ---\\n\\n\"\n",
    "    \"Based on the provided information, please answer the following question:\"\n",
    ").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4f071-c41a-4e39-933b-0748ae86e2ef",
   "metadata": {},
   "source": [
    "### 4. Define Function to Invoke a Bedrock Model with Caching and Display Usage Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97d480-17b8-43ce-9b8a-9dbdbd9fc842",
   "metadata": {},
   "source": [
    "In this step, a function is defined to send a crafted prompt to the `Amazon Nova Lite` model on `Amazon Bedrock`, combining static and dynamic content with a cache point marker. It records the model's response time, prints usage metrics from the response, and displays the generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d008a0-c5cf-41e3-994c-736e6eb1e093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "\n",
    "def invoke_model_and_print_all_metrics(static_content, dynamic_content, invocation_number=1):\n",
    "    print(f\"\\n--- Invoking Bedrock Model: {model_id} (Invocation {invocation_number}) ---\")\n",
    "    \n",
    "    messages_content = [\n",
    "        {\"text\": static_content},\n",
    "        {\"cachePoint\": {\"type\": \"default\"}}, # Always include cachePoint for caching behavior\n",
    "        {\"text\": dynamic_content}\n",
    "    ]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": messages_content\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    body = {\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 2048,\n",
    "            \"temperature\": 0.1,\n",
    "            \"topP\": 0.9\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=body[\"messages\"],\n",
    "            inferenceConfig=body[\"inferenceConfig\"]\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        latency = (end_time - start_time) * 1000 # in milliseconds\n",
    "\n",
    "        usage = response.get(\"usage\", {})\n",
    "        \n",
    "        print(f\"Response Latency: {latency:.2f} ms\")\n",
    "        print(\"--- Metrics from Usage Field ---\")\n",
    "        if usage:\n",
    "            for metric_name, metric_value in usage.items():\n",
    "                print(f\"- {metric_name}: {metric_value}\")\n",
    "        else:\n",
    "            print(\"No usage metrics found in the response.\")\n",
    "        print(\"------------------------------------\\n\")\n",
    "\n",
    "        # Print model response snippet\n",
    "        model_response_text = \"\"\n",
    "        if 'output' in response and 'message' in response['output'] and 'content' in response['output']['message']:\n",
    "            for content_block in response['output']['message']['content']:\n",
    "                if 'text' in content_block:\n",
    "                    model_response_text += content_block['text']\n",
    "        print(f\"Model response: {model_response_text.strip()[:200]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking model (Invocation {invocation_number}): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2aa2f7-faba-4593-a95c-8c4b168412d6",
   "metadata": {},
   "source": [
    "### 5. Invoke the Model and Display Token Usage and Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256df36-8399-4c45-97d9-3c631a775865",
   "metadata": {},
   "source": [
    "This step invokes the model twice: the first call creates a cache entry, while the second call retrieves the response from the cache, resulting in a cache hit. Because the second call uses the cached response, it incurs lower costs as no new model inference is performed. You can see the difference by comparing the `cacheReadInputTokens` and `cacheWriteInputTokens` values printed after each call, along with the model’s responses. The cache TTL is 5 minutes, so keep this in mind if you run the code multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4e0f6-6426-4d8d-a556-e102c3553e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Execute the two model invocations ---\n",
    "# First invocation: This will attempt to write the static_content to cache.\n",
    "# Define the dynamic user inquiry\n",
    "user_inquiry = \"How is my information used according to your privacy statement?\"\n",
    "invoke_model_and_print_all_metrics(static_prompt_content, user_inquiry, invocation_number=1)\n",
    "\n",
    "# Small delay before the second invocation\n",
    "time.sleep(3) \n",
    "\n",
    "# Second invocation: This will attempt to read the static_content from cache.\n",
    "# Define the dynamic user inquiry\n",
    "user_inquiry = \"How do you handle my information, based on your privacy policy?\"\n",
    "invoke_model_and_print_all_metrics(static_prompt_content, user_inquiry, invocation_number=2)\n",
    "\n",
    "print(\"\\n--------------------\")\n",
    "print(\"The output above shows all metrics returned in the 'usage' field for both model invocations.\")\n",
    "print(\"Observe 'cacheWriteInputTokens' for the first invocation and 'cacheReadInputTokens' for the second.\")\n",
    "print(\"Also compare the latency in miliseconds for each execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a84d0ca",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully completed objective two **Implement Response Caching to Reduce Costs**. You can now close the `objective_two.ipynb` file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
