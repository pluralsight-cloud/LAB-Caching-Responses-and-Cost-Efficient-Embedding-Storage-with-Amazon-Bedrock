{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9771e561-a128-40d2-8ce0-a91f356786f7",
   "metadata": {},
   "source": [
    "# Caching Responses and Cost-efficient Embedding Storage with Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90371b78",
   "metadata": {},
   "source": [
    "## Objective Three: Choose an Efficient Storage Strategy for Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a78f6",
   "metadata": {},
   "source": [
    "In this hands-on walkthrough, you’ll use the `boto3 Python SDK` to interact with Amazon Bedrock and generate text embeddings using the `amazon.titan-embed-text-v2:0` model. You’ll create embeddings from a set of meal recipes and explore different storage and search strategies, including in-memory storage, `FAISS` for vector indexing, `Amazon S3` for persistent object storage, and `PostgreSQL` with the `pgvector` extension for SQL-based similarity search. Each approach demonstrates how to store and retrieve embeddings efficiently, along with its performance characteristics and associated storage costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c6c59",
   "metadata": {},
   "source": [
    "### 1. Prepare the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b4cb7d",
   "metadata": {},
   "source": [
    "This step includes the code to install the required `Python` packages needed for the rest of the exercise and restart the kernel to ensure the packages are properly loaded. While running, you might see some `pip` dependency warnings. These can be safely ignored as they won’t impact the steps we’re performing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859bafa5-ba1f-49fd-bd9a-099c3df2571c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"✅ Please wait while the installation completes. This may take a few \"\n",
    "      \"minutes. If you encounter any dependency errors, you can ignore them.\")\n",
    "%pip install --upgrade -q botocore\n",
    "%pip install --upgrade -q boto3\n",
    "%pip install -q numpy==1.26.4\n",
    "%pip install psycopg2-binary\n",
    "%conda install -y -q -c conda-forge faiss-cpu\n",
    "print(\"✅ Installation completed!\")\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "try:\n",
    "    display(HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\"))\n",
    "    print(\"✅ Kernel restarted successfully\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to restart the kernel\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2aa284",
   "metadata": {},
   "source": [
    "In this step, you import several libraries required for embedding generation, storage, and similarity search. You use `boto3` to interact with AWS services programmatically and `json` to handle JSON formatting for requests and responses. The `faiss` library is used to build and search a vector index in memory, enabling fast similarity lookups. `numpy` is imported to handle numerical operations and vector transformations required for cosine similarity calculations. Finally, `psycopg2` is used to connect to a `PostgreSQL` database and interact with it when storing and querying embeddings using the `pgvector` extension. You also create a Bedrock client using `boto3`, which allows you to invoke models from the `Amazon Bedrock` service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad998b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import boto3\n",
    "    import json\n",
    "    import faiss\n",
    "    import numpy as np\n",
    "    import psycopg2\n",
    "    print(\"----------------------------\")\n",
    "    print(\"✅ Libraries loaded successfully.\")\n",
    "except ImportError as e:\n",
    "    print(\"----------------------------\")\n",
    "    print(\"❌ Failed to load libraries.\")\n",
    "    print(f\"Error: {e}\")\n",
    "try:\n",
    "    client = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=\"us-east-1\"\n",
    "    )\n",
    "    print(\"✅ bedrock-runtime client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to initialize bedrock-runtime client.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166af2a8",
   "metadata": {},
   "source": [
    "### 2. Define Sample Recipes for Embedding Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65271a",
   "metadata": {},
   "source": [
    "In this step you  define a set of recipes stored in a `Python` dictionary. Each entry in the dictionary includes the name of the recipe as the key and its full description as the value. These recipes will be used as input to generate embeddings for different storage and search scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfcbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recipes = {\n",
    "        \"Spaghetti Carbonara\": \"\"\"Boil spaghetti. In a pan, cook pancetta until crispy.\n",
    "        Beat eggs with parmesan cheese. Combine spaghetti, pancetta, and egg mixture.\n",
    "        Stir quickly to create a creamy sauce. Serve hot.\"\"\",\n",
    "\n",
    "        \"Chicken Curry\": \"\"\"Cook chopped onions, garlic, and ginger in oil. \n",
    "        Add curry powder, cumin, and turmeric. Stir in chicken pieces and brown\n",
    "        them. Add tomatoes and simmer until chicken is cooked through.\n",
    "        Serve with rice.\"\"\",\n",
    "\n",
    "        \"Vegan Salad\": \"\"\"Mix chopped kale, spinach, cherry tomatoes, and cucumbers.\n",
    "        Add avocado slices and chickpeas. Dress with lemon juice, olive oil, and salt.\"\"\",\n",
    "\n",
    "        \"Grilled Cheese Sandwich\": \"\"\"Butter two slices of bread. Place cheddar cheese\n",
    "        between them. Grill in a pan until bread is golden and cheese is melted.\"\"\"\n",
    "    }\n",
    "print(\"✅ Dictionary created succesfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62435ed",
   "metadata": {},
   "source": [
    "### 3. Define Function to Generate Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f130e",
   "metadata": {},
   "source": [
    "This step defines the function to request `Amazon Bedrock` to invoke the `amazon.titan-embed-text-v2:0` model. It prepares the input text as a `JSON` payload, sends the request using the `Bedrock` client, and parses the response to extract the embedding. The resulting embedding is returned for use in storage or similarity search scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f543f51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    payload = {\"inputText\": text}\n",
    "    response = client.invoke_model(\n",
    "        modelId=\"amazon.titan-embed-text-v2:0\",\n",
    "        body=json.dumps(payload),\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    result = json.loads(response['body'].read())\n",
    "    return result[\"embedding\"]\n",
    "\n",
    "print(\"✅ Function to invoke model and create embeddings has been defined succesfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4396b9",
   "metadata": {},
   "source": [
    "### 4. Defining the Cosine Similarity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92171404",
   "metadata": {},
   "source": [
    "This step defines a function to calculate cosine similarity between two vectors. It converts the input lists to `NumPy` arrays and computes the cosine of the angle between them using the dot product and vector norms. The result is a value between -1 and 1, where higher values indicate greater similarity. This function will be used in scenarios where the storage solution does not provide built-in similarity search capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f6d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "print(\"✅ Cosine similarity function has been created succesfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670fa89c-db54-43c4-b0e5-256cde8e98b6",
   "metadata": {},
   "source": [
    "### 5. Store Embeddings in Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac71fc",
   "metadata": {},
   "source": [
    "This method stores the recipe embeddings in memory and defines a function to perform a semantic similarity search. It uses the previously defined cosine similarity function to compare the query embedding with the in-memory stored embeddings and returns the most similar recipe based on the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e1937-cc9b-4c7d-8a18-bc1212bc2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embeddings in memory\n",
    "embedding_store = {}\n",
    "\n",
    "for name, text in recipes.items():\n",
    "    embedding = get_embedding(text)\n",
    "    embedding_store[name] = {\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "\n",
    "# Perform semantic similarity search\n",
    "def search_similar_recipes(query, top_k=1):\n",
    "    query_embedding = get_embedding(query)\n",
    "    results = []\n",
    "\n",
    "    for name, data in embedding_store.items():\n",
    "        score = cosine_similarity(query_embedding, data[\"embedding\"])\n",
    "        results.append((name, score, data[\"text\"]))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "# Example query\n",
    "query = \"How do I cook spaghetti with eggs, cheese, and pancetta?\"\n",
    "results = search_similar_recipes(query)\n",
    "\n",
    "# Display results\n",
    "for name, score, text in results:\n",
    "    print(f\"🔹Recipe: {name}\")\n",
    "    print(f\"   Similarity Score: {score:.3f}\")\n",
    "    print(f\"   Description: {text.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249bb285",
   "metadata": {},
   "source": [
    "Storing embeddings in memory is fast and cost-free, making it ideal for quick lookups and small-scale applications. It avoids the complexity of setting up external storage and offers low-latency access. However, it's volatile—data is lost when the application stops—and it doesn't scale well, as memory is limited and not suitable for storing large volumes of embeddings over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901651b9",
   "metadata": {},
   "source": [
    "### 6. Store Embeddings in FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a0553",
   "metadata": {},
   "source": [
    "This block generates normalized embeddings for each recipe and stores them in a `FAISS` index configured for cosine similarity. It defines a function that embeds the user’s query, normalizes it, and retrieves the most semantically similar recipe using inner product search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding for one sample recipe to determine embedding dimension\n",
    "sample_embedding = get_embedding(next(iter(recipes.values())))\n",
    "embedding_dim = len(sample_embedding)\n",
    "print(f\"Detected embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Create FAISS index for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "\n",
    "# Store recipe names and texts for mapping\n",
    "recipe_lookup = []\n",
    "embedding_matrix = []\n",
    "\n",
    "for name, text in recipes.items():\n",
    "    embedding = get_embedding(text)\n",
    "    embedding_matrix.append(embedding)\n",
    "    recipe_lookup.append((name, text))\n",
    "\n",
    "# Convert to NumPy array and normalize for cosine similarity\n",
    "embedding_matrix = np.array(embedding_matrix).astype('float32')\n",
    "faiss.normalize_L2(embedding_matrix)  # normalize each vector to unit length\n",
    "\n",
    "# Add embeddings to FAISS index\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# Semantic Search with FAISS using cosine similarity\n",
    "def search_faiss(query, top_k=1):\n",
    "    query_embedding = np.array(get_embedding(query)).astype('float32').reshape(1, -1)\n",
    "    faiss.normalize_L2(query_embedding)  # normalize query vector\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx, score in zip(indices[0], scores[0]):\n",
    "        name, recipe_text = recipe_lookup[idx]\n",
    "        results.append((name, score, recipe_text))\n",
    "    return results\n",
    "\n",
    "# Example query\n",
    "query = \"How do I cook chicken with onions?\"\n",
    "results = search_faiss(query)\n",
    "\n",
    "# Display results\n",
    "for name, score, text in results:\n",
    "    print(f\"🔹 Recipe: {name}\")\n",
    "    print(f\"   Similarity Score: {score:.3f} (higher is more similar)\")\n",
    "    print(f\"   Description: {text.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22647aa6",
   "metadata": {},
   "source": [
    "Using `FAISS` in RAM offers fast and cost-effective similarity search with no external storage costs. Unlike plain in-memory storage, it supports large datasets through efficient indexing and can scale better. `FAISS` also supports disk-based indexes for persistence. However, in-memory use is still volatile and limited by available memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34635a",
   "metadata": {},
   "source": [
    "### 7. Store Embeddings in Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc3c11",
   "metadata": {},
   "source": [
    "This block generates embeddings for each recipe and stores them as `JSON` objects in an Amazon `S3` bucket. Since `S3` doesn't provide built-in similarity search capabilities, it uses the previously defined cosine similarity function to compare a query embedding against the stored embeddings. The top matching recipe is returned based on semantic relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b80726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get dynamic AWS context\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.session.Session().region_name\n",
    "bucket_name = f\"recipes-embeddings-bucket-{account_id}-{region}\"\n",
    "embedding_prefix = \"recipes/\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "# Generate and store embeddings in S3\n",
    "for name, text in recipes.items():\n",
    "    embedding = get_embedding(text)\n",
    "    data = {\n",
    "        \"recipe_name\": name,\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "    key = f\"{embedding_prefix}{name.replace(' ', '_')}.json\"\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=json.dumps(data),\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "# Load embeddings from S3\n",
    "def load_all_embeddings():\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=embedding_prefix)\n",
    "    results = []\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        file_obj = s3.get_object(Bucket=bucket_name, Key=obj[\"Key\"])\n",
    "        body = file_obj[\"Body\"].read()\n",
    "        data = json.loads(body)\n",
    "        results.append(data)\n",
    "    return results\n",
    "\n",
    "# Search using cosine similarity\n",
    "def search_similar_recipe(query, top_k=1):  \n",
    "    query_embedding = get_embedding(query)\n",
    "    stored_items = load_all_embeddings()\n",
    "\n",
    "    similarities = []\n",
    "    for item in stored_items:\n",
    "        score = cosine_similarity(query_embedding, item[\"embedding\"])\n",
    "        similarities.append((item[\"recipe_name\"], score, item[\"text\"]))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Example query\n",
    "query = \"How do I prepare a vegan salad?\"\n",
    "results = search_similar_recipe(query)\n",
    "\n",
    "# Display results\n",
    "for name, score, text in results:\n",
    "    print(f\"🔹 Recipe: {name}\")\n",
    "    print(f\"   Similarity Score: {score:.3f}\")\n",
    "    print(f\"   Description: {text.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1b7e2-94e9-4a9c-b2e8-c784e98ea6a4",
   "metadata": {},
   "source": [
    "Using `Amazon S3` to store embeddings offers durability, scalability, and low long-term storage. It persists data across sessions and is ideal for batch processing or infrequently accessed embeddings. Compared to in-memory strategies, `S3` incurs cost per request and data volume stored, and it's slower for real-time search due to network latency and the need to load data before querying. However, it scales far beyond memory limits and supports easy data sharing across services or notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec371c",
   "metadata": {},
   "source": [
    "### 8. Store Embeddings in PostgreSQL DB with `pgvector` Vector Extension "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04416a56",
   "metadata": {},
   "source": [
    "This block connects to a `PostgreSQL` database, generates embeddings for a set of recipe descriptions, and inserts them into a table that supports vector search using the `pgvector` extension. It then performs a similarity search by embedding a user query and retrieving the top relevant recipe, ordered by distance from the query embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd30e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DB_INSTANCE_ID = \"recipes-pg-db\"\n",
    "DB_NAME = \"recipesdb\"\n",
    "DB_PORT = 5432\n",
    "DB_USER = \"masterusername\"\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "rds = boto3.client(\"rds\", region_name=region)\n",
    "sts = boto3.client(\"sts\", region_name=region)\n",
    "\n",
    "# Get RDS endpoint\n",
    "db_info = rds.describe_db_instances(DBInstanceIdentifier=DB_INSTANCE_ID)\n",
    "db_endpoint = db_info['DBInstances'][0]['Endpoint']['Address']\n",
    "\n",
    "# Get DB password from AWS account ID\n",
    "account_id = sts.get_caller_identity()['Account']\n",
    "db_password = account_id\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=db_endpoint,\n",
    "    port=DB_PORT,\n",
    "    database=DB_NAME,\n",
    "    user=DB_USER,\n",
    "    password=db_password,\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Ensure pgvector is installed\n",
    "cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "conn.commit()\n",
    "\n",
    "# Create recipes table with vector(1024) if not exist\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS recipes (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        name TEXT UNIQUE,\n",
    "        description TEXT,\n",
    "        embedding vector(1024)\n",
    "    );\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# Insert embeddings only if not already present\n",
    "for name, desc in recipes.items():\n",
    "    cursor.execute(\"SELECT 1 FROM recipes WHERE name = %s\", (name,))\n",
    "    if cursor.fetchone():\n",
    "        continue\n",
    "    embedding = get_embedding(desc)  # Your embedding function must return 1024-dim vector\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO recipes (name, description, embedding) VALUES (%s, %s, %s)\",\n",
    "        (name, desc, embedding)\n",
    "    )\n",
    "conn.commit()\n",
    "\n",
    "# Perform semantic similarity search\n",
    "query = \"How do I cook a cheese sandwich?\"\n",
    "query_embedding = get_embedding(query)\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT name, description, embedding <=> %s::vector AS cosine_distance\n",
    "    FROM recipes\n",
    "    ORDER BY cosine_distance ASC\n",
    "    LIMIT 1;\n",
    "\"\"\", (query_embedding,))\n",
    "\n",
    "results = cursor.fetchall()\n",
    "for name, description, distance in results:\n",
    "    print(f\"🔹 Recipe: {name}\")\n",
    "    print(f\"   Distance: {distance:.3f}\")\n",
    "    print(f\"   Description: {description.strip()}\\n\")\n",
    "\n",
    "# Display results\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e14b7",
   "metadata": {},
   "source": [
    "Databases with vector support provide persistent, indexed storage and efficient similarity search, making them well-suited for real-time embedding queries and structured data. They handle larger datasets better than memory but require more setup and can incur higher costs as data and query volume grow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea0bac",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully completed objective two **Choose an Efficient Storage Strategy for Embeddings**. You can now close the `objective_three.ipynb` file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
