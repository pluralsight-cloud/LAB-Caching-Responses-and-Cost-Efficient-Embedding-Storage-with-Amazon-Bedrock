{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9771e561-a128-40d2-8ce0-a91f356786f7",
   "metadata": {},
   "source": [
    "# Caching Responses and Cost-efficient Embedding Storage with Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90371b78",
   "metadata": {},
   "source": [
    "## Objective Three: Choose an Efficient Storage Strategy for Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a78f6",
   "metadata": {},
   "source": [
    "In this hands-on walkthrough, you‚Äôll use the boto3 Python SDK to interact with Amazon Bedrock and generate text embeddings using the `amazon.titan-embed-text-v2:0` model. You‚Äôll create embeddings from a set of meal recipes and explore different storage and search strategies, including in-memory storage, FAISS for vector indexing, Amazon S3 for persistent object storage, and PostgreSQL with the pgvector extension for SQL-based similarity search. Each approach demonstrates how to store and retrieve embeddings efficiently, along with its performance characteristics and associated storage costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c6c59",
   "metadata": {},
   "source": [
    "### 1. Prepare the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b4cb7d",
   "metadata": {},
   "source": [
    "This step includes the code to install the required Python packages needed for the rest of the exercise and restart the kernel to ensure the packages are properly loaded. While running, you might see some pip dependency warnings. These can be safely ignored as they won‚Äôt impact the steps we‚Äôre performing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "859bafa5-ba1f-49fd-bd9a-099c3df2571c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Please wait for the installation to complete. This may take a few minutes.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: psycopg2-binary in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.9.9)\n",
      "Requirement already satisfied: psycopg2==2.9.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from psycopg2-binary) (2.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - conda-forge\n",
      " - nvidia\n",
      " - pytorch\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Continue to next step to restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Please wait for the installation to complete. This may take a few minutes.\")\n",
    "%pip install --upgrade -q botocore\n",
    "%pip install --upgrade -q boto3\n",
    "%pip install -q numpy==1.26.4\n",
    "%pip install psycopg2-binary\n",
    "%conda install -y -q -c conda-forge faiss-cpu\n",
    "print(\"‚úÖ Continue to next step to restart the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e301b03-2d56-4430-89fe-471bcd227bfc",
   "metadata": {},
   "source": [
    "#### Restart the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b66a19-5fb0-4b64-be9b-95160fbda48a",
   "metadata": {},
   "source": [
    "Restart the kernel for changes to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1096498-969b-4c98-b11d-db5743dd9cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "try:\n",
    "    display(HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\"))\n",
    "    print(\"‚úÖ Kernel restarted successfully\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to restart the kernel\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfd2e3-2c85-4163-a743-d31ceed400f4",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2aa284",
   "metadata": {},
   "source": [
    "In this step, you import several libraries required for embedding generation, storage, and similarity search. You use `boto3` to interact with AWS services programmatically and `json` to handle JSON formatting for requests and responses. The `faiss` library is used to build and search a vector index in memory, enabling fast similarity lookups. `numpy` is imported to handle numerical operations and vector transformations required for cosine similarity calculations. Finally, `psycopg2` is used to connect to a PostgreSQL database and interact with it when storing and querying embeddings using the pgvector extension. You also create a Bedrock client using boto3, which allows you to invoke models from the Amazon Bedrock service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad998b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import boto3\n",
    "    import json\n",
    "    import faiss\n",
    "    import numpy as np\n",
    "    import psycopg2\n",
    "    print(\"----------------------------\")\n",
    "    print(\"‚úÖ Libraries loaded successfully.\")\n",
    "except ImportError as e:\n",
    "    print(\"----------------------------\")\n",
    "    print(\"‚ùå Failed to load libraries.\")\n",
    "    print(f\"Error: {e}\")\n",
    "try:\n",
    "    client = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=\"us-east-1\"\n",
    "    )\n",
    "    print(\"‚úÖ bedrock-runtime client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to initialize bedrock-runtime client.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166af2a8",
   "metadata": {},
   "source": [
    "### 2. Sample Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65271a",
   "metadata": {},
   "source": [
    "In this step you  define a set of recipes stored in a Python dictionary. Each entry in the dictionary includes the name of the recipe as the key and its full description as the value. These recipes will be used as input to generate embeddings for different storage and search scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfcbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recipes = {\n",
    "        \"Spaghetti Carbonara\": \"\"\"Boil spaghetti. In a pan, cook pancetta until crispy.\n",
    "        Beat eggs with parmesan cheese. Combine spaghetti, pancetta, and egg mixture.\n",
    "        Stir quickly to create a creamy sauce. Serve hot.\"\"\",\n",
    "\n",
    "        \"Chicken Curry\": \"\"\"Cook chopped onions, garlic, and ginger in oil. \n",
    "        Add curry powder, cumin, and turmeric. Stir in chicken pieces and brown\n",
    "        them. Add tomatoes and simmer until chicken is cooked through.\n",
    "        Serve with rice.\"\"\",\n",
    "\n",
    "        \"Vegan Salad\": \"\"\"Mix chopped kale, spinach, cherry tomatoes, and cucumbers.\n",
    "        Add avocado slices and chickpeas. Dress with lemon juice, olive oil, and salt.\"\"\",\n",
    "\n",
    "        \"Grilled Cheese Sandwich\": \"\"\"Butter two slices of bread. Place cheddar cheese\n",
    "        between them. Grill in a pan until bread is golden and cheese is melted.\"\"\"\n",
    "    }\n",
    "print(\"‚úÖ Dictionary created succesfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62435ed",
   "metadata": {},
   "source": [
    "### 3. Invoke Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f130e",
   "metadata": {},
   "source": [
    "This step defines the function to request Amazon Bedrock to invoke the `amazon.titan-embed-text-v2:0` model. It prepares the input text as a JSON payload, sends the request using the Bedrock client, and parses the response to extract the embedding. The resulting embedding is returned for use in storage or similarity search scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f543f51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    payload = {\"inputText\": text}\n",
    "    response = client.invoke_model(\n",
    "        modelId=\"amazon.titan-embed-text-v2:0\",\n",
    "        body=json.dumps(payload),\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    result = json.loads(response['body'].read())\n",
    "    return result[\"embedding\"]\n",
    "\n",
    "\n",
    "print(\"‚úÖ Function to invoke model and create embeddings has been defined succesfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4396b9",
   "metadata": {},
   "source": [
    "### 4. Defining the Cosine Similarity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92171404",
   "metadata": {},
   "source": [
    "This step defines a function to calculate cosine similarity between two vectors. It converts the input lists to NumPy arrays, then computes the cosine of the angle between them using the dot product and vector norms. The result is a value between -1 and 1, where higher values indicate greater similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f6d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Cosine similarity function has been created succesfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670fa89c-db54-43c4-b0e5-256cde8e98b6",
   "metadata": {},
   "source": [
    "### 5. Store Embeddings in Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac71fc",
   "metadata": {},
   "source": [
    "This method stores the recipe embeddings in memory and defines a function to perform a semantic similarity search. It generates an embedding for the input query, compares it with stored embeddings using cosine similarity, and returns the most similar recipe based on the highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e1937-cc9b-4c7d-8a18-bc1212bc2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embeddings in memory\n",
    "embedding_store = {}\n",
    "\n",
    "for name, text in recipes.items():\n",
    "    embedding = get_embedding(text)\n",
    "    embedding_store[name] = {\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "\n",
    "\n",
    "# Perform semantic similarity search\n",
    "def search_similar_recipes(query, top_k=1):\n",
    "    query_embedding = get_embedding(query)\n",
    "    results = []\n",
    "\n",
    "    for name, data in embedding_store.items():\n",
    "        score = cosine_similarity(query_embedding, data[\"embedding\"])\n",
    "        results.append((name, score, data[\"text\"]))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "\n",
    "# Run a search for Spaghetti Carbonara-like recipe\n",
    "query = \"How do I cook spaghetti with eggs, cheese, and pancetta?\"\n",
    "results = search_similar_recipes(query)\n",
    "\n",
    "# Print results\n",
    "for name, score, text in results:\n",
    "    print(f\"üîπRecipe: {name}\")\n",
    "    print(f\"   Similarity Score: {score:.3f}\")\n",
    "    print(f\"   Description: {text.strip()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249bb285",
   "metadata": {},
   "source": [
    "Storing embeddings in memory is fast and cost-free, making it ideal for quick lookups and small-scale applications. It avoids the complexity of setting up external storage and offers low-latency access. However, it's volatile‚Äîdata is lost when the application stops‚Äîand it doesn't scale well, as memory is limited and not suitable for storing large volumes of embeddings over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901651b9",
   "metadata": {},
   "source": [
    "### 6. Store Embeddings in FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a0553",
   "metadata": {},
   "source": [
    "This block generates normalized embeddings for each recipe and stores them in a FAISS index configured for cosine similarity. It defines a function that embeds the user‚Äôs query, normalizes it, and retrieves the most semantically similar recipe using inner product search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding for one sample recipe to determine embedding dimension\n",
    "sample_embedding = get_embedding(next(iter(recipes.values())))\n",
    "embedding_dim = len(sample_embedding)\n",
    "print(f\"Detected embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Create FAISS index for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "\n",
    "# Store recipe names and texts for mapping\n",
    "recipe_lookup = []\n",
    "embedding_matrix = []\n",
    "\n",
    "for name, text in recipes.items():\n",
    "    embedding = get_embedding(text)\n",
    "    embedding_matrix.append(embedding)\n",
    "    recipe_lookup.append((name, text))\n",
    "\n",
    "# Convert to NumPy array and normalize for cosine similarity\n",
    "embedding_matrix = np.array(embedding_matrix).astype('float32')\n",
    "faiss.normalize_L2(embedding_matrix)  # normalize each vector to unit length\n",
    "\n",
    "# Add embeddings to FAISS index\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "\n",
    "# Semantic Search with FAISS using cosine similarity\n",
    "def search_faiss(query, top_k=1):\n",
    "    query_embedding = np.array(get_embedding(query)).astype('float32').reshape(1, -1)\n",
    "    faiss.normalize_L2(query_embedding)  # normalize query vector\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx, score in zip(indices[0], scores[0]):\n",
    "        name, recipe_text = recipe_lookup[idx]\n",
    "        results.append((name, score, recipe_text))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example query\n",
    "query = \"How do I cook chicken with onions?\"\n",
    "results = search_faiss(query)\n",
    "\n",
    "# Display results\n",
    "for name, score, text in results:\n",
    "    print(f\"üîπ Recipe: {name}\")\n",
    "    print(f\"   Similarity Score: {score:.3f} (higher is more similar)\")\n",
    "    print(f\"   Description: {text.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22647aa6",
   "metadata": {},
   "source": [
    "Using FAISS in RAM offers fast and cost-effective similarity search with no external storage costs. Unlike plain in-memory storage, it supports large datasets through efficient indexing and can scale better. FAISS also supports disk-based indexes for persistence. However, in-memory use is still volatile and limited by available memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34635a",
   "metadata": {},
   "source": [
    "### 7. Store Embeddings in Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc3c11",
   "metadata": {},
   "source": [
    "This block generates embeddings for each recipe and stores them as JSON objects in an Amazon S3 bucket. It defines a function to load all stored embeddings from S3 and another to compare a query embedding against them using cosine similarity. The top matching recipe is returned based on semantic relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b80726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get dynamic AWS context\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.session.Session().region_name\n",
    "bucket_name = f\"recipes-embeddings-bucket-{account_id}-{region}\"\n",
    "embedding_prefix = \"recipes/\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "# Generate and store embeddings in S3\n",
    "for name, text in recipes.items():\n",
    "    embedding = get_embedding(text)\n",
    "    data = {\n",
    "        \"recipe_name\": name,\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "    key = f\"{embedding_prefix}{name.replace(' ', '_')}.json\"\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=json.dumps(data),\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Load embeddings from S3\n",
    "def load_all_embeddings():\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=embedding_prefix)\n",
    "    results = []\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        file_obj = s3.get_object(Bucket=bucket_name, Key=obj[\"Key\"])\n",
    "        body = file_obj[\"Body\"].read()\n",
    "        data = json.loads(body)\n",
    "        results.append(data)\n",
    "    return results\n",
    "\n",
    "\n",
    "def search_similar_recipe(query, top_k=1):  # top_k=1 to return only top match\n",
    "    query_embedding = get_embedding(query)\n",
    "    stored_items = load_all_embeddings()\n",
    "\n",
    "    similarities = []\n",
    "    for item in stored_items:\n",
    "        score = cosine_similarity(query_embedding, item[\"embedding\"])\n",
    "        similarities.append((item[\"recipe_name\"], score, item[\"text\"]))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "\n",
    "# Example query\n",
    "query = \"How do I prepare a vegan salad?\"\n",
    "results = search_similar_recipe(query)\n",
    "\n",
    "# Show results\n",
    "for name, score, text in results:\n",
    "    print(f\"üîπ Recipe: {name}\")\n",
    "    print(f\"   Similarity Score: {score:.3f}\")\n",
    "    print(f\"   Description: {text.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1b7e2-94e9-4a9c-b2e8-c784e98ea6a4",
   "metadata": {},
   "source": [
    "Using Amazon S3 to store embeddings offers durability, scalability, and low long-term storage cost. It persists data across sessions and is ideal for batch processing or infrequently accessed embeddings. Compared to in-memory strategies, S3 incurs cost per request and data volume stored, and it's slower for real-time search due to network latency and the need to load data before querying. However, it scales far beyond memory limits and supports easy data sharing across services or notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec371c",
   "metadata": {},
   "source": [
    "### 8. Store Embeddings in PostgreSQL DB with `pgvector` Vector Extension "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04416a56",
   "metadata": {},
   "source": [
    "This block connects to a PostgreSQL database, generates embeddings for a set of recipe descriptions, and inserts them into a table that supports vector search using the `pgvector` extension. It then performs a similarity search by embedding a user query and retrieving the top relevant recipe, ordered by distance from the query embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd30e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Get AWS region from environment (set by SageMaker automatically)\n",
    "aws_region = os.environ.get('AWS_REGION')\n",
    "if not aws_region:\n",
    "    raise Exception(\"AWS_REGION environment variable not found\")\n",
    "\n",
    "# Get DB instance identifier from environment variable for flexibility\n",
    "db_instance_id = os.environ.get('DB_INSTANCE_ID', 'recipesdb-instance')\n",
    "\n",
    "# Get RDS endpoint dynamically\n",
    "rds_client = boto3.client('rds', region_name=aws_region)\n",
    "response = rds_client.describe_db_instances(DBInstanceIdentifier=db_instance_id)\n",
    "host = response['DBInstances'][0]['Endpoint']['Address']\n",
    "port = response['DBInstances'][0]['Endpoint']['Port']\n",
    "dbname = os.environ.get('DB_NAME', 'recipesdb')\n",
    "\n",
    "# Get IAM DB user from environment (must be a PostgreSQL user configured for IAM auth)\n",
    "db_user = os.environ.get('DB_USER')\n",
    "if not db_user:\n",
    "    raise Exception(\"DB_USER environment variable not set\")\n",
    "\n",
    "# Generate IAM auth token for PostgreSQL connection\n",
    "rds_iam_client = boto3.client('rds', region_name=aws_region)\n",
    "auth_token = rds_iam_client.generate_db_auth_token(\n",
    "    DBHostname=host,\n",
    "    Port=port,\n",
    "    DBUsername=db_user,\n",
    "    Region=aws_region\n",
    ")\n",
    "\n",
    "# Connect to PostgreSQL with IAM token as password\n",
    "conn = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=port,\n",
    "    dbname=dbname,\n",
    "    user=db_user,\n",
    "    password=auth_token,\n",
    "    sslmode='require'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Enable pgvector extension if not exists\n",
    "cursor.execute(\"CREATE EXTENSION IF NOT EXISTS pgvector;\")\n",
    "conn.commit()\n",
    "\n",
    "# Create recipes table if not exists\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS recipes (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        description TEXT,\n",
    "        embedding vector(1536)\n",
    "    );\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def get_embedding(text):\n",
    "    # Replace with actual embedding generation, e.g. Bedrock call\n",
    "    import numpy as np\n",
    "    return list(np.random.rand(1536))  # dummy vector\n",
    "\n",
    "recipes = {\n",
    "    \"Spaghetti Carbonara\": \"Classic Italian pasta with eggs, cheese, pancetta, and pepper.\",\n",
    "    \"Tomato Soup\": \"Rich and creamy tomato soup with basil and cream.\"\n",
    "}\n",
    "\n",
    "# Insert embeddings\n",
    "for name, text in recipes.items():\n",
    "    embedding = get_embedding(text)\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO recipes (name, description, embedding) VALUES (%s, %s, %s)\",\n",
    "        (name, text, embedding)\n",
    "    )\n",
    "conn.commit()\n",
    "\n",
    "query = \"How do I cook spaghetti with eggs, cheese, and pancetta?\"\n",
    "query_embedding = get_embedding(query)\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT name, description, embedding <-> %s AS distance\n",
    "    FROM recipes\n",
    "    ORDER BY distance ASC\n",
    "    LIMIT 2;\n",
    "\"\"\", (query_embedding,))\n",
    "\n",
    "results = cursor.fetchall()\n",
    "for name, description, distance in results:\n",
    "    print(f\"üîπ Recipe: {name}\")\n",
    "    print(f\"   Distance: {distance:.3f}\")\n",
    "    print(f\"   Description: {description.strip()}\\n\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
