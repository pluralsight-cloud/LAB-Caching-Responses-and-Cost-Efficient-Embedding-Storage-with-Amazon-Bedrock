{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9771e561-a128-40d2-8ce0-a91f356786f7",
   "metadata": {},
   "source": [
    "# Caching Responses and Cost-efficient Embedding Storage with Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90371b78",
   "metadata": {},
   "source": [
    "## Objective Three: Choose an Efficient Storage Strategy for Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a78f6",
   "metadata": {},
   "source": [
    "In this hands-on walkthrough, you’ll use the boto3 Python SDK to interact with Amazon Bedrock and generate text embeddings using the `amazon.titan-embed-text-v2:0` model. You’ll create embeddings from a set of meal recipes and explore different storage and search strategies, including in-memory storage, FAISS for vector indexing, Amazon S3 for persistent object storage, and PostgreSQL with the pgvector extension for SQL-based similarity search. Each approach demonstrates how to store and retrieve embeddings efficiently, along with its performance characteristics and associated storage costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c6c59",
   "metadata": {},
   "source": [
    "### 1. Prepare the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b4cb7d",
   "metadata": {},
   "source": [
    "This step includes the code to install the required Python packages needed for the rest of the exercise and restart the kernel to ensure the packages are properly loaded. While running, you might see some pip dependency warnings. These can be safely ignored as they won’t impact the steps we’re performing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859bafa5-ba1f-49fd-bd9a-099c3df2571c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade -q botocore\n",
    "%pip install --upgrade -q boto3\n",
    "%pip install -q numpy==1.26.4\n",
    "%pip install psycopg2-binary\n",
    "%conda install -y -q -c conda-forge faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e301b03-2d56-4430-89fe-471bcd227bfc",
   "metadata": {},
   "source": [
    "#### Restart the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b66a19-5fb0-4b64-be9b-95160fbda48a",
   "metadata": {},
   "source": [
    "Restart the kernel for changes to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1096498-969b-4c98-b11d-db5743dd9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "try:\n",
    "    display(HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\"))\n",
    "    print(\"✅ Kernel restarted successfully\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to restart the kernel\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfd2e3-2c85-4163-a743-d31ceed400f4",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2aa284",
   "metadata": {},
   "source": [
    "In this step, you import several libraries required for embedding generation, storage, and similarity search. You use `boto3` to interact with AWS services programmatically and `json` to handle JSON formatting for requests and responses. The `faiss` library is used to build and search a vector index in memory, enabling fast similarity lookups. `numpy` is imported to handle numerical operations and vector transformations required for cosine similarity calculations. Finally, `psycopg2` is used to connect to a PostgreSQL database and interact with it when storing and querying embeddings using the pgvector extension. You also create a Bedrock client using boto3, which allows you to invoke models from the Amazon Bedrock service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import boto3\n",
    "    import json\n",
    "    import faiss\n",
    "    import numpy as np\n",
    "    import psycopg2\n",
    "    print(\"----------------------------\")\n",
    "    print(\"✅ Libraries loaded successfully.\")\n",
    "except ImportError as e:\n",
    "    print(\"----------------------------\")\n",
    "    print(\"❌ Failed to load libraries.\")\n",
    "    print(f\"Error: {e}\")\n",
    "try:\n",
    "    client = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=\"us-east-1\"\n",
    "    )\n",
    "    print(\"✅ bedrock-runtime client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to initialize bedrock-runtime client.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166af2a8",
   "metadata": {},
   "source": [
    "### 2. Sample Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65271a",
   "metadata": {},
   "source": [
    "In this step you  define a set of recipes stored in a Python dictionary. Each entry in the dictionary includes the name of the recipe as the key and its full description as the value. These recipes will be used as input to generate embeddings for different storage and search scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfcbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = {\n",
    "        \"Spaghetti Carbonara\": \"\"\"Boil spaghetti. In a pan, cook pancetta until crispy.\n",
    "        Beat eggs with parmesan cheese. Combine spaghetti, pancetta, and egg mixture.\n",
    "        Stir quickly to create a creamy sauce. Serve hot.\"\"\",\n",
    "        \n",
    "        \"Chicken Curry\": \"\"\"Cook chopped onions, garlic, and ginger in oil. Add curry powder,\n",
    "        cumin, and turmeric. Stir in chicken pieces and brown them. Add tomatoes and\n",
    "        simmer until chicken is cooked through. Serve with rice.\"\"\",\n",
    "        \n",
    "        \"Vegan Salad\": \"\"\"Mix chopped kale, spinach, cherry tomatoes, and cucumbers.\n",
    "        Add avocado slices and chickpeas. Dress with lemon juice, olive oil, and salt.\"\"\",\n",
    "        \n",
    "        \"Grilled Cheese Sandwich\": \"\"\"Butter two slices of bread. Place cheddar cheese\n",
    "        between them. Grill in a pan until bread is golden and cheese is melted.\"\"\"\n",
    "    }\n",
    "print(\"✅ Dictionary created succesfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62435ed",
   "metadata": {},
   "source": [
    "### 3. Invoke Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f130e",
   "metadata": {},
   "source": [
    "This step defines the function to request Amazon Bedrock to invoke the `amazon.titan-embed-text-v2:0` model. It prepares the input text as a JSON payload, sends the request using the Bedrock client, and parses the response to extract the embedding. The resulting embedding is returned for use in storage or similarity search scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f543f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    payload = {\"inputText\": text}\n",
    "    response = client.invoke_model(\n",
    "        modelId=\"amazon.titan-embed-text-v2:0\",\n",
    "        body=json.dumps(payload),\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    result = json.loads(response['body'].read())\n",
    "    return result[\"embedding\"]\n",
    "\n",
    "\n",
    "print(\"✅ Function to invoke model and create embeddings has been defined succesfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4396b9",
   "metadata": {},
   "source": [
    "### 4. Defining the Cosine Similarity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92171404",
   "metadata": {},
   "source": [
    "This step defines a function to calculate cosine similarity between two vectors. It converts the input lists to NumPy arrays, then computes the cosine of the angle between them using the dot product and vector norms. The result is a value between -1 and 1, where higher values indicate greater similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f6d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "print(\"✅ Cosine similarity function has been created succesfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670fa89c-db54-43c4-b0e5-256cde8e98b6",
   "metadata": {},
   "source": [
    "### 5. Store Embeddings in Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac71fc",
   "metadata": {},
   "source": [
    "This method stores the recipe embeddings in memory and defines a function to perform a semantic similarity search. It generates an embedding for the input query, compares it with stored embeddings using cosine similarity, and returns the most similar recipe based on the highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e1937-cc9b-4c7d-8a18-bc1212bc2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embeddings in memory\n",
    "embedding_store = {}\n",
    "\n",
    "for name, text in recipes.items():\n",
    "    embedding = get_embedding(text)\n",
    "    embedding_store[name] = {\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "\n",
    "\n",
    "# Perform semantic similarity search\n",
    "def search_similar_recipes(query, top_k=1):\n",
    "    query_embedding = get_embedding(query)\n",
    "    results = []\n",
    "\n",
    "    for name, data in embedding_store.items():\n",
    "        score = cosine_similarity(query_embedding, data[\"embedding\"])\n",
    "        results.append((name, score, data[\"text\"]))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "\n",
    "# Run a search for Spaghetti Carbonara-like recipe\n",
    "query = \"How do I cook spaghetti with eggs, cheese, and pancetta?\"\n",
    "results = search_similar_recipes(query)\n",
    "\n",
    "# Print results\n",
    "for name, score, text in results:\n",
    "    print(f\"🔹Recipe: {name}\")\n",
    "    print(f\"   Similarity Score: {score:.3f}\")\n",
    "    print(f\"   Description: {text.strip()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249bb285",
   "metadata": {},
   "source": [
    "Storing embeddings in memory is fast and cost-free, making it ideal for quick lookups and small-scale applications. It avoids the complexity of setting up external storage and offers low-latency access. However, it's volatile—data is lost when the application stops—and it doesn't scale well, as memory is limited and not suitable for storing large volumes of embeddings over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901651b9",
   "metadata": {},
   "source": [
    "### 6. Store Embeddings in FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a0553",
   "metadata": {},
   "source": [
    "This block generates normalized embeddings for each recipe and stores them in a FAISS index configured for cosine similarity. It defines a function that embeds the user’s query, normalizes it, and retrieves the most semantically similar recipe using inner product search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings and build FAISS index using cosine similarity (inner product with normalized vectors)\n",
    "embedding_dim = 1536\n",
    "index = faiss.IndexFlatIP(embedding_dim)  # IP = inner product ≈ cosine similarity for normalized vectors\n",
    "\n",
    "# Store recipe names and texts for mapping\n",
    "recipe_lookup = []\n",
    "embedding_matrix = []\n",
    "\n",
    "for name, text in recipes.items():\n",
    "    embedding = get_embedding(text)\n",
    "    embedding_matrix.append(embedding)\n",
    "    recipe_lookup.append((name, text))\n",
    "\n",
    "# Convert to NumPy array and normalize for cosine similarity\n",
    "embedding_matrix = np.array(embedding_matrix).astype('float32')\n",
    "faiss.normalize_L2(embedding_matrix)  # normalize each vector to unit length\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# Semantic Search with FAISS using cosine similarity\n",
    "def search_faiss(query, top_k=2):\n",
    "    query_embedding = np.array(get_embedding(query)).astype('float32').reshape(1, -1)\n",
    "    faiss.normalize_L2(query_embedding)  # normalize query too\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx, score in zip(indices[0], distances[0]):\n",
    "        name, recipe_text = recipe_lookup[idx]\n",
    "        results.append((name, score, recipe_text))\n",
    "    return results\n",
    "\n",
    "# Example query\n",
    "query = \"How do I cook spaghetti with eggs, cheese, and pancetta?\"\n",
    "results = search_faiss(query)\n",
    "\n",
    "# Display results\n",
    "for name, score, text in results:\n",
    "    print(f\"🔹 Recipe: {name}\")\n",
    "    print(f\"   Similarity Score: {score:.3f} (higher is more similar)\")\n",
    "    print(f\"   Description: {text.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22647aa6",
   "metadata": {},
   "source": [
    "Using FAISS in RAM offers fast and cost-effective similarity search with no external storage costs. Unlike plain in-memory storage, it supports large datasets through efficient indexing and can scale better. FAISS also supports disk-based indexes for persistence. However, in-memory use is still volatile and limited by available memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
